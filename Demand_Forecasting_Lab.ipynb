{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "943bb9da-9e8d-4ea0-9e6a-49d759e80d3c",
          "showTitle": false,
          "title": ""
        },
        "id": "05I-ngdE0KGG"
      },
      "source": [
        "# Demand Forecasting Lab\n",
        "In this notebook, you are going to implement end-to-end Demand Forecasting solution including data processing, feature computation, model training and validation. You learn how to use the pyspark ML objects on big data.\n",
        "\n",
        "## Environment"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gdown\n",
        "gdown.download_folder('https://drive.google.com/drive/folders/1Yv66Ng1IHcGlafsT96wWPpdRRs-sRo6F?usp=sharing', quiet=True)\n",
        "gdown.download_folder('https://drive.google.com/drive/folders/1TnTEFGYWpmj6qqYJId8LHT_ow6bfavdC?usp=drive_link', quiet=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CxMPWYNv0u3N",
        "outputId": "b5fe21b0-592a-4137-daa5-59e5eeb590fb"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/Product_Demand_Validation_All/_committed_7611924776083353318',\n",
              " '/content/Product_Demand_Validation_All/_started_7611924776083353318',\n",
              " '/content/Product_Demand_Validation_All/_SUCCESS (1)',\n",
              " '/content/Product_Demand_Validation_All/part-00000-tid-7611924776083353318-639b66f7-dd8e-4fb0-8013-b7aa1757ed46-247-1-c000.snappy.parquet']"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# installing jdk\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "#downloading .tgz installation file for Spache spark\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-3.5.0/spark-3.5.0-bin-hadoop3.tgz\n",
        "# installing apache spark from downloaded file\n",
        "!tar xf spark-3.5.0-bin-hadoop3.tgz\n",
        "# installing findspark library\n",
        "!pip install -q findspark\n",
        "import os\n",
        "import findspark\n",
        "#setting up paths for JDK and spark\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.0-bin-hadoop3\"\n",
        "#initiating findspark\n",
        "findspark.init()"
      ],
      "metadata": {
        "id": "eJMZb6UiLBzl"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "34c4584a-e733-4d33-b4dd-ec67548c919f",
          "showTitle": false,
          "title": "Environment"
        },
        "id": "3dWLZead0KGH"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql import Window as W\n",
        "from pyspark.sql import types as T\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "from pyspark.ml.regression import GBTRegressor\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "5755206f-67c3-49c6-a5f5-5f88b4cef899",
          "showTitle": false,
          "title": ""
        },
        "id": "javdJ65I0KGJ"
      },
      "source": [
        "## Load data\n",
        "Read data with past daily demand for individual products. The dataset contains historical product demand for a sales company. The company offers thousands of products within dozens of product categories. It normally takes up to one month to deliver their products after ordering, thus monthly demand forecasts would be beneficial to the company in multiple ways.\n",
        "\n",
        "### Data format\n",
        "- Product_Code (string): Product identification number\n",
        "- Product_Category (string): Product category identifier\n",
        "- Date (date)\n",
        "- Demand (int): Quantity of products"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.appName(\"SparkDayThree\").getOrCreate()"
      ],
      "metadata": {
        "id": "GeSP1MPAOaZL"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "db2537f5-7209-48fc-be38-bf23535dbb89",
          "showTitle": false,
          "title": "Load data"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5rH2dfR0KGJ",
        "outputId": "8c42cb4a-57f7-47f7-fdc6-e889420c0c79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------+------------+----------+-------+\n",
            "|Product_Category|Product_Code|      Date| Demand|\n",
            "+----------------+------------+----------+-------+\n",
            "|    Category_005|Product_1581|2012-02-29|18000.0|\n",
            "|    Category_005|Product_1581|2012-07-09| 1000.0|\n",
            "|    Category_011|Product_0664|2012-03-01| 1296.0|\n",
            "|    Category_007|Product_1023|2012-05-24|10600.0|\n",
            "|    Category_006|Product_0929|2012-06-27| 1000.0|\n",
            "+----------------+------------+----------+-------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_raw = spark.read.parquet(\"/content/Product_Demand_Train_Test_All\")\n",
        "\n",
        "df_raw.show(5)\n",
        "# Explore data format and sample values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "4901e670-adb3-4214-a7c9-565931ed6e34",
          "showTitle": false,
          "title": ""
        },
        "id": "JLNKzSeb0KGJ"
      },
      "source": [
        "# Milestone #1: Data analysis & Features\n",
        "In this section start analysing the raw data and prepare a final dataset for your model.\n",
        "\n",
        "## Objectives\n",
        "You should\n",
        "- Compute some descriptive statistics\n",
        "- Clean the raw data\n",
        "- Perform target (Demand) analysis and create target variable for demand forecasting on monthly basis\n",
        "- Analyse category distribution and other related characteristics\n",
        "- Create at least two features\n",
        "\n",
        "## Milestone presentation outline\n",
        "1. Show at least one interesting finding from data analysis\n",
        "2. Explain target variable definition and its characteristics\n",
        "3. Prepare your modelling features (and modelling dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "66d43c96-8a7e-44eb-bf85-61bf2b8ab15c",
          "showTitle": false,
          "title": ""
        },
        "id": "dkuYHlYq0KGJ",
        "outputId": "f18866cc-1131-4d4d-85ec-430bdec8540e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------+------------+----------+-------+----+-----+---+-----------+-------+\n",
            "|Product_Category|Product_Code|      Date| Demand|Year|Month|Day|Day_of_week|Quarter|\n",
            "+----------------+------------+----------+-------+----+-----+---+-----------+-------+\n",
            "|    Category_005|Product_1581|2012-02-29|18000.0|2012|    2| 29|          4|      1|\n",
            "|    Category_005|Product_1581|2012-07-09| 1000.0|2012|    7|  9|          2|      3|\n",
            "|    Category_011|Product_0664|2012-03-01| 1296.0|2012|    3|  1|          5|      1|\n",
            "|    Category_007|Product_1023|2012-05-24|10600.0|2012|    5| 24|          5|      2|\n",
            "|    Category_006|Product_0929|2012-06-27| 1000.0|2012|    6| 27|          4|      2|\n",
            "+----------------+------------+----------+-------+----+-----+---+-----------+-------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import year, month, dayofmonth, dayofweek, quarter\n",
        "df_processed = df_raw.withColumn(\"Year\", year(\"Date\")) \\\n",
        "                    .withColumn(\"Month\", month(\"Date\")) \\\n",
        "                    .withColumn(\"Day\", dayofmonth(\"Date\")) \\\n",
        "                    .withColumn(\"Day_of_week\", dayofweek(\"Date\")) \\\n",
        "                    .withColumn(\"Quarter\", quarter(\"Date\"))\n",
        "\n",
        "# Show the resulting dataframe\n",
        "df_processed.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "589RwkejnIxg",
        "outputId": "1b7c5c16-a5b1-4f30-a551-43e1eab14573",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 606
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `01` cannot be resolved. Did you mean one of the following? [`year`, `month`, `avg_demand`].;\n'Project [year#460, month#470, avg_demand#490, concat_ws(-, year#460, month#470, '01) AS date_str#494]\n+- Aggregate [year#460, month#470], [year#460, month#470, avg(Demand#414) AS avg_demand#490]\n   +- Project [Product_Category#411, Product_Code#412, Date#413, Demand#414, year#460, month(Date#413) AS month#470, day#433, day_of_week#441, quarter#450]\n      +- Project [Product_Category#411, Product_Code#412, Date#413, Demand#414, year(Date#413) AS year#460, month#426, day#433, day_of_week#441, quarter#450]\n         +- Project [Product_Category#411, Product_Code#412, Date#413, Demand#414, year#419, month#426, day#433, day_of_week#441, quarter(Date#413) AS quarter#450]\n            +- Project [Product_Category#411, Product_Code#412, Date#413, Demand#414, year#419, month#426, day#433, dayofweek(Date#413) AS day_of_week#441]\n               +- Project [Product_Category#411, Product_Code#412, Date#413, Demand#414, year#419, month#426, dayofmonth(Date#413) AS day#433]\n                  +- Project [Product_Category#411, Product_Code#412, Date#413, Demand#414, year#419, month(Date#413) AS month#426]\n                     +- Project [Product_Category#411, Product_Code#412, Date#413, Demand#414, year(Date#413) AS year#419]\n                        +- Relation [Product_Category#411,Product_Code#412,Date#413,Demand#414] parquet\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-ee6c86c29b7b>\u001b[0m in \u001b[0;36m<cell line: 27>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m# Concatenate year and month columns to create a date string for plotting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m df_monthly_avg = df_monthly_avg.withColumn(\"date_str\", \n\u001b[0m\u001b[1;32m     28\u001b[0m                                            concat_ws(\"-\", \"year\", \"month\", \"01\"))\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.5.0-bin-hadoop3/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mwithColumn\u001b[0;34m(self, colName, col)\u001b[0m\n\u001b[1;32m   5168\u001b[0m                 \u001b[0mmessage_parameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"arg_name\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"col\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"arg_type\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5169\u001b[0m             )\n\u001b[0;32m-> 5170\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkSession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5172\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwithColumnRenamed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexisting\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"DataFrame\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.5.0-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.5.0-bin-hadoop3/python/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `01` cannot be resolved. Did you mean one of the following? [`year`, `month`, `avg_demand`].;\n'Project [year#460, month#470, avg_demand#490, concat_ws(-, year#460, month#470, '01) AS date_str#494]\n+- Aggregate [year#460, month#470], [year#460, month#470, avg(Demand#414) AS avg_demand#490]\n   +- Project [Product_Category#411, Product_Code#412, Date#413, Demand#414, year#460, month(Date#413) AS month#470, day#433, day_of_week#441, quarter#450]\n      +- Project [Product_Category#411, Product_Code#412, Date#413, Demand#414, year(Date#413) AS year#460, month#426, day#433, day_of_week#441, quarter#450]\n         +- Project [Product_Category#411, Product_Code#412, Date#413, Demand#414, year#419, month#426, day#433, day_of_week#441, quarter(Date#413) AS quarter#450]\n            +- Project [Product_Category#411, Product_Code#412, Date#413, Demand#414, year#419, month#426, day#433, dayofweek(Date#413) AS day_of_week#441]\n               +- Project [Product_Category#411, Product_Code#412, Date#413, Demand#414, year#419, month#426, dayofmonth(Date#413) AS day#433]\n                  +- Project [Product_Category#411, Product_Code#412, Date#413, Demand#414, year#419, month(Date#413) AS month#426]\n                     +- Project [Product_Category#411, Product_Code#412, Date#413, Demand#414, year(Date#413) AS year#419]\n                        +- Relation [Product_Category#411,Product_Code#412,Date#413,Demand#414] parquet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "cc08c6da-6a2d-496f-803e-8a9ddb27afb2",
          "showTitle": false,
          "title": ""
        },
        "id": "2XwdR_Ik0KGK"
      },
      "source": [
        "## Expected Results\n",
        "You should end up with a cleaned dataset with well defined target variable and at least two features for your demand forecasting model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "b55f3b2b-36a1-48c7-96c5-29f9474c04eb",
          "showTitle": false,
          "title": ""
        },
        "id": "ljoakVeP0KGK"
      },
      "source": [
        "# Milestone #2: Modelling & Experiment design\n",
        "Create first model on the dataset from previous section.\n",
        "## Objectives\n",
        "You should\n",
        "- Propose train and test division of the data\n",
        "- Create a modelling pipeline using spark ML and [GBTRegressor](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.regression.GBTRegressor.html)\n",
        "- Train first model\n",
        "- Obtain first predictions on test set\n",
        "## Milestone presentation outline\n",
        "1. Describe your train and test datasets\n",
        "2. Show your modelling pipeline and code for training your data\n",
        "3. Provide first predictions on the test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "e85186f8-f958-406f-8ed6-a5f6d62217f5",
          "showTitle": false,
          "title": ""
        },
        "id": "7zI4ywGk0KGK"
      },
      "outputs": [],
      "source": [
        "# your code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "92e75364-b8a7-42c7-8e3d-4cc1c15a7901",
          "showTitle": false,
          "title": ""
        },
        "id": "5hiu3Ldp0KGK"
      },
      "source": [
        "# Milestone #3: Tuning & Evaluation\n",
        "This section is dedicated to improving your model performance and evaluation.\n",
        "## Objectives\n",
        "You should\n",
        "- Perform hyperparameter tunning (by hand or using [CrossValidator](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.tuning.CrossValidator.html))\n",
        "- Evaluate feature importance\n",
        "- Calculate standard evaluation metrics on your test set and on the provided validation set\n",
        "## Milestone presentation outline\n",
        "1. Explain your choice of hyperparameters and your approach to tunning\n",
        "2. Provide feature importance\n",
        "3. Summarize overall model performance using the evaluation metrics on your test set and the provided validation set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "3e8e8b12-1129-48ec-89b0-60a6066e6c1d",
          "showTitle": false,
          "title": ""
        },
        "id": "3flbgs0o0KGK"
      },
      "outputs": [],
      "source": [
        "# your code for model tunning and feature imprtance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "5d69b9c4-10c3-4ecc-a418-d7729b36ea4b",
          "showTitle": false,
          "title": ""
        },
        "id": "x53wLRmM0KGK"
      },
      "source": [
        "## Expected Results\n",
        "Final model for monthly demand forecasting ready for out of sample predictions.\n",
        "\n",
        "## Out of sample validation\n",
        "Test your model on a hold out sample that was not previously available in the dataset. As you can see below you will provide demand predictions for December 2016."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "facbcc20-e8d5-40b8-9c14-46eeeba528b4",
          "showTitle": false,
          "title": ""
        },
        "id": "ubNMJksN0KGK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "069593c6-ad6c-4042-c280-5b54de53fdc2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+----+-----+--------+\n",
            "|Product_Code|Year|Month|  Demand|\n",
            "+------------+----+-----+--------+\n",
            "|Product_1383|2016|   12| 18354.0|\n",
            "|Product_0767|2016|   12|  3857.0|\n",
            "|Product_1439|2016|   12|217582.0|\n",
            "|Product_1762|2016|   12|    14.0|\n",
            "|Product_0346|2016|   12|    11.0|\n",
            "+------------+----+-----+--------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Get Demand for validation data\n",
        "df_raw = spark.read.parquet(\"/content/Product_Demand_Validation_All\")\n",
        "df_raw.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "aa4aa14c-f7ad-4ced-a436-c24b4c28e6f5",
          "showTitle": false,
          "title": ""
        },
        "id": "_9OZfYkP0KGL"
      },
      "source": [
        "Transform the raw data in the same way as your original dataset and obtain predictions using your final model. There should be a pyspark data frame `predictions_val` with columns\n",
        "- Product_Code\n",
        "- Demand\n",
        "- Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "deff06d6-e5ec-4dd0-a165-025405732872",
          "showTitle": false,
          "title": ""
        },
        "id": "paixrS5x0KGL"
      },
      "outputs": [],
      "source": [
        "# your code to obtain predictions and compose a final dataset called \"predictions_val\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "f7ee998d-6e0e-43e5-9b43-dbaa4ee7461d",
          "showTitle": false,
          "title": ""
        },
        "id": "PObQmrYh0KGL"
      },
      "source": [
        "The code below provides standard metrics for model evaluation - Root Mean Squared Error and Mean Absolute Error as a final model characteristics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "c1bfc3f5-7ee3-4c51-adcc-bc02a9605ff8",
          "showTitle": false,
          "title": ""
        },
        "id": "F2Sk0THe0KGL"
      },
      "outputs": [],
      "source": [
        "# Calculate final metrics for comparison\n",
        "predictions_val = (\n",
        "    predictions_val\n",
        "        .withColumn(\"Error\", F.col(\"Demand\") - F.col(\"Prediction\"))\n",
        "        .withColumn(\"AbsError\", F.abs(F.col(\"Demand\") - F.col(\"Prediction\")))\n",
        "        .withColumn(\"SqrError\", F.pow(F.col(\"Demand\") - F.col(\"Prediction\"), F.lit(2)))\n",
        ")\n",
        "\n",
        "mae = predictions_val.agg(F.avg(F.col(\"AbsError\"))).collect()[0][0]\n",
        "print(\"Mean Absolute Error (MAE) on validation data = %g\" % mae)\n",
        "rmse = predictions_val.agg(F.sqrt(F.avg(F.col(\"SqrError\")))).collect()[0][0]\n",
        "print(\"Root Mean Squared Error (RMSE) on validation data = %g\" % rmse)"
      ]
    }
  ],
  "metadata": {
    "application/vnd.databricks.v1+notebook": {
      "dashboards": [],
      "language": "python",
      "notebookMetadata": {
        "pythonIndentUnit": 4
      },
      "notebookName": "Demand Forecasting Lab",
      "widgets": {}
    },
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}